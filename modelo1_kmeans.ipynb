import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar los datos desde el archivo CSV
data = pd.read_csv("Mall_Customers.csv")

# Mostrar las primeras filas del DataFrame para entender la estructura de los datos
print("Primeras filas del DataFrame:")
print(data.head())

# Obtener información general sobre el DataFrame
print("\nInformación general sobre los datos:")
print(data.info())

# Estadísticas descriptivas de las variables numéricas
print("\nEstadísticas descriptivas de las variables numéricas:")
print(data.describe())

# Verificar la presencia de valores nulos o faltantes
print("\nValores nulos por columna:")
print(data.isnull().sum())

# Verificar si hay duplicados en los datos
print("\nNúmero de filas duplicadas:", data.duplicated().sum())

# Visualizar la distribución de las variables numéricas mediante histogramas
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
sns.histplot(data['Age'], bins=20, kde=True, color='skyblue')
plt.title('Distribución de Edades')

plt.subplot(1, 3, 2)
sns.histplot(data['Annual Income (k$)'], bins=20, kde=True, color='salmon')
plt.title('Distribución de Ingresos Anuales')

plt.subplot(1, 3, 3)
sns.histplot(data['Spending Score (1-100)'], bins=20, kde=True, color='green')
plt.title('Distribución de Puntuación de Gasto')

plt.tight_layout()
plt.show()

# Visualizar la distribución de género
plt.figure(figsize=(6, 4))
sns.countplot(data['Gender'], palette='pastel')
plt.title('Distribución de Género')
plt.xlabel('Género')
plt.ylabel('Frecuencia')
plt.show()

# En este conjunto de datos no hay valores faltantes, por lo que no es necesario imputar

# En este conjunto de datos, 'Gender' es la única variable categórica que ya está codificada como 'Male' y 'Female'

# En este caso, no es necesario normalizar o estandarizar las variables para K-means

# No hay columnas no relevantes en este conjunto de datos

# Mostrar el DataFrame después del preprocesamiento
print("DataFrame después del preprocesamiento:")
print(data.head())

# No se requiere selección de características para K-means, ya que utiliza todas las características disponibles

print("No se requiere selección de características para K-means.")

from sklearn.cluster import KMeans

# Seleccionar las características relevantes para el modelo (en este caso, todas las características numéricas)
X = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]

# Configurar los hiperparámetros del modelo K-means
n_clusters = 5  # Número de clústeres
random_state = 42  # Semilla aleatoria para reproducibilidad

# Inicializar y entrenar el modelo K-means
kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
kmeans.fit(X)

# Obtener las etiquetas de clúster para cada muestra
cluster_labels = kmeans.labels_

# Mostrar las etiquetas de clúster asignadas a cada muestra
print("Etiquetas de clúster asignadas a cada muestra:")
print(cluster_labels)

from sklearn.metrics import silhouette_score, calinski_harabasz_score

# Calcular el Coeficiente de Silhouette
silhouette_score_value = silhouette_score(X, cluster_labels)
print("Coeficiente de Silhouette:", silhouette_score_value)

# Calcular el Índice de Calinski-Harabasz
calinski_harabasz_score_value = calinski_harabasz_score(X, cluster_labels)
print("Índice de Calinski-Harabasz:", calinski_harabasz_score_value)

# Obtener la inercia del modelo K-means
inertia_value = kmeans.inertia_
print("Inercia del modelo K-means:", inertia_value)

# Calcular la dispersión media dentro del clúster
cluster_centers = kmeans.cluster_centers_
cluster_distances = []
for i, center in enumerate(cluster_centers):
    cluster_indices = (cluster_labels == i)
    cluster_samples = X[cluster_indices]
    distances = ((cluster_samples - center) ** 2).sum(axis=1)
    cluster_distances.append(distances.mean())
average_cluster_distance = sum(cluster_distances) / len(cluster_distances)
print("Dispersión media dentro del clúster:", average_cluster_distance)

# Visualizar los centroides de los clústeres
plt.figure(figsize=(8, 6))
plt.scatter(X['Annual Income (k$)'], X['Spending Score (1-100)'], c=cluster_labels, cmap='viridis', s=50, alpha=0.5)
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='X', s=200, label='Centroides')
plt.title('Clustering K-means')
plt.xlabel('Ingreso Anual (k$)')
plt.ylabel('Puntuación de Gasto (1-100)')
plt.legend()
plt.grid(True)
plt.show()

# Visualizar los clústeres en función de las tres características
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X['Age'], X['Annual Income (k$)'], X['Spending Score (1-100)'], c=cluster_labels, cmap='viridis', s=50)
ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], cluster_centers[:, 2], c='red', marker='X', s=200, label='Centroides')
ax.set_xlabel('Edad')
ax.set_ylabel('Ingreso Anual (k$)')
ax.set_zlabel('Puntuación de Gasto (1-100)')
plt.title('Clustering K-means en 3D')
plt.legend()
plt.show()

# Interpretación y análisis de los resultados

# La inercia del modelo indica cuán cerca están los puntos dentro de cada clúster de su respectivo centroide.
# Valores de inercia más bajos indican clústeres más compactos.
print("La inercia del modelo K-means es:", inertia_value)

# El Coeficiente de Silhouette proporciona una medida de cuán bien separados están los clústeres.
# Valores más cercanos a 1 indican una buena separación entre clústeres.
print("Coeficiente de Silhouette:", silhouette_score_value)

# El Índice de Calinski-Harabasz es otra medida de cuán bien separados están los clústeres.
# Valores más altos indican clústeres más compactos y bien separados.
print("Índice de Calinski-Harabasz:", calinski_harabasz_score_value)

# La dispersión media dentro del clúster proporciona una medida de la cohesión de cada clúster.
# Valores más bajos indican clústeres más cohesionados.
print("Dispersión media dentro del clúster:", average_cluster_distance)

# Basándonos en las visualizaciones, podemos observar cómo se agrupan los clientes en función de sus características.
# Por ejemplo, podemos identificar grupos de clientes con bajos ingresos y baja puntuación de gasto, 
# grupos de clientes con ingresos moderados y alta puntuación de gasto, etc.

# También podemos utilizar esta información para segmentar el mercado y adaptar estrategias de marketing específicas para cada segmento.

# Es importante tener en cuenta que el número de clústeres (5 en este caso) fue elegido de antemano, pero podría ser ajustado 
# utilizando técnicas como el método del codo o el método de la silueta para encontrar el número óptimo de clústeres.

# En resumen, el modelo de clustering K-means ha sido entrenado con éxito y proporciona una segmentación útil de los clientes 
# en función de sus características de edad, ingreso anual y puntuación de gasto.




