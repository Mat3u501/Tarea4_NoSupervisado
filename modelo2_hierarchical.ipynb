import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Cargar los datos
data = pd.read_csv('MallCustomers.csv')

# Visualizar las primeras filas
print("Primeras filas del conjunto de datos:")
print(data.head())

# Información general del conjunto de datos
print("\nInformación general del conjunto de datos:")
print(data.info())

# Resumen estadístico de las variables numéricas
print("\nResumen estadístico de las variables numéricas:")
print(data.describe())

# Verificar valores faltantes
print("\nValores faltantes por columna:")
print(data.isnull().sum())

# Visualización de la distribución de las variables
plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
sns.histplot(data['Age'], kde=True, color='skyblue')
plt.title('Distribución de la Edad')

plt.subplot(2, 2, 2)
sns.histplot(data['Annual Income (k$)'], kde=True, color='salmon')
plt.title('Distribución del Ingreso Anual')

plt.subplot(2, 2, 3)
sns.histplot(data['Spending Score (1-100)'], kde=True, color='green')
plt.title('Distribución de la Puntuación de Gasto')

plt.tight_layout()
plt.show()

# Visualizar la relación entre las variables
plt.figure(figsize=(12, 6))
sns.pairplot(data)
plt.show()

# Tratamiento de valores faltantes (si los hubiera)
# Comprobamos si hay valores faltantes
print("\nValores faltantes antes del preprocesamiento:")
print(data.isnull().sum())

# No hay valores faltantes en este conjunto de datos, por lo que no es necesario realizar imputaciones

# Transformaciones de variables categóricas (si las hubiera)
# En este conjunto de datos no hay variables categóricas que requieran transformación

# Normalización o estandarización de las variables (si es necesario)
# En este caso, como las variables tienen diferentes escalas, es recomendable estandarizarlas
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data_scaled = scaler.fit_transform(data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])

# Convertir el resultado a un DataFrame
data_scaled = pd.DataFrame(data_scaled, columns=['Age', 'Annual Income (k$)', 'Spending Score (1-100)'])

# Visualizar las primeras filas del DataFrame estandarizado
print("\nPrimeras filas del DataFrame estandarizado:")
print(data_scaled.head())

# Importar librerías necesarias
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# Seleccionar las características más relevantes usando la prueba F de Fisher
selector = SelectKBest(score_func=f_regression, k=2)
X_selected = selector.fit_transform(data_scaled, data['Gender'])

# Obtener los índices de las características seleccionadas
selected_indices = selector.get_support(indices=True)

# Obtener los nombres de las características seleccionadas
selected_features = data_scaled.columns[selected_indices]

print("\nCaracterísticas seleccionadas:")
print(selected_features)

# Importar la clase para clustering jerárquico
from sklearn.cluster import AgglomerativeClustering

# Inicializar el modelo de clustering
model = AgglomerativeClustering(n_clusters=5, linkage='ward')

# Entrenar el modelo con los datos
model.fit(data_scaled)

# Obtener las etiquetas de los clusters
cluster_labels = model.labels_

from sklearn.metrics import silhouette_score, calinski_harabasz_score

# Calcular el Coeficiente de Silhouette
silhouette_coefficient = silhouette_score(data_scaled, cluster_labels)
print("Coeficiente de Silhouette:", silhouette_coefficient)

# Calcular el Índice de Calinski-Harabasz
calinski_harabasz_index = calinski_harabasz_score(data_scaled, cluster_labels)
print("Índice de Calinski-Harabasz:", calinski_harabasz_index)

# Importar la función pairwise_distances de scikit-learn
from sklearn.metrics import pairwise_distances

# Calcular la matriz de distancias entre todas las muestras
distances = pairwise_distances(data_scaled)

# Calcular la dispersión intra-cluster
intra_cluster_dispersion = np.sum(np.min(distances, axis=1)) / data_scaled.shape[0]
print("Dispersión Intra-Cluster:", intra_cluster_dispersion)

# Calcular la dispersión inter-cluster
inter_cluster_dispersion = np.mean(np.min(distances, axis=0))
print("Dispersión Inter-Cluster:", inter_cluster_dispersion)

# Visualizar los resultados del clustering
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data_scaled['Annual Income (k$)'], y=data_scaled['Spending Score (1-100)'], hue=cluster_labels, palette='viridis')
plt.title('Clustering Jerárquico')
plt.xlabel('Ingreso Anual (k$)')
plt.ylabel('Puntuación de Gasto (1-100)')
plt.legend(title='Cluster')
plt.show()

# La visualización resultante muestra la agrupación de los clientes en función de sus ingresos anuales y puntuación de gasto. 
# Cada punto en el gráfico representa a un cliente, y el color indica a qué cluster pertenece según el modelo de clustering jerárquico.

# Al observar la distribución de los clusters, podemos notar que hay segmentos de clientes que tienen características similares en términos de ingresos anuales y puntuación de gasto. 
#Por ejemplo, podríamos identificar claramente grupos de clientes que tienen ingresos anuales altos pero una puntuación de gasto baja, así como grupos con ingresos más bajos pero una puntuación de gasto alta. 
#Estos hallazgos podrían ser útiles para la estrategia de marketing y la gestión de la tienda, ya que podrían sugerir diferentes enfoques para atraer y retener a diferentes segmentos de clientes.
